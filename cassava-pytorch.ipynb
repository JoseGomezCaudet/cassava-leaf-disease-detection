{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{},"cell_type":"markdown","source":"TODO: \n- Unir los datos de ambas competiciones\n- Intentar política one cycle\n- Más data augmentation\n- Sin RandomCrop\n- Con el data augmentation de brillo y saturación"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import *\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom skimage import io, transform\nfrom PIL import Image\nimport PIL\nimport cv2\n\nimport pandas as pd\nimport numpy as np\n\nfrom pathlib import Path\nimport os \nimport random\n\npath1 = Path(\"../input/cassava-leaf-disease-classification\")\npath2 = Path(\"../input/cassava-leaf-disease-merged\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(path/\"train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = [len(data[data.label == i]) for i in np.sort(data.label.unique())]\nstate = [\"Bacterias\", \"Veta marrón\", \"Moteado verde\", \"Mosaico\", \"Sano\"]\nfor i,j in enumerate(count):\n    print(str(state[i]) + \": \" + str(j) + \", lo que supone un \" + str(round(j*100/len(data), 2)) + \"% del total.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_oh = pd.concat([data.image_id, pd.get_dummies(data[\"label\"])], axis=1)\ndata_oh.columns = [\"image_id\", \"Bacterias\", \"Veta marrón\", \"Moteado verde\", \"Mosaico\", \"Sano\"]\ndata_oh.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_oh.to_csv(\"train_oh.csv\", index_label=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_from_csv(path_csv, pct_split):\n    total_data = pd.read_csv(path_csv)\n    # Borrar si hay solo una columna\n    total_data = total_data.drop('source', 1)\n    valid_data = total_data.sample(frac=pct_split)\n    idxs = list(set(list(range(len(total_data)))) - set(valid_data.index))\n    train_data = total_data.iloc[idxs]\n\n    return train_data.reset_index(drop=True), valid_data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"t, v = split_from_csv(\"./train_oh.csv\", 0.2)\nlen(t), len(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t, v = split_from_csv(path2/\"merged.csv\", 0.2)\nlen(t), len(v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a dataset class, which preload all images from a pandas dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset_from_csv(Dataset):\n    \"\"\"\n    Dataset creado teniendo las imágenes en una carpeta y las etiquetas en un \n    csv en el que se mencionan los nombres de las imágenes de entrada a las que \n    corresponde cada etiqueta.\n    \"\"\"\n    def __init__(self, path, dataframe, dir_images, transforms=None):\n        \"\"\"\n        Args:\n            path (Path Object): Ruta del directorio principal\n            csv_file (string): Nombre del csv.\n            dir_images (string): Ruta a la carpeta con las imágenes.\n            transforms (callable, optional): Lista de posibles transformaciones\n                hecha con Compose. Debe acabar con ToTensor.\n        \"\"\"\n        self.path = path\n        #dataframe =  dataframe[:(int(len(dataframe)/10))] # Temporal\n        self.labels = dataframe\n        self.path_images = dir_images\n        self.transforms = transforms\n\n        \"\"\"#Cargamos las imágenes para facilitar la velocidad del entrenamiento\n        self.loaded_images = {}\n        count = 0\n        print(\"Inicio de la carga de archivos.\")\n        for name in dataframe.image_id:\n            if (count%500)==1: print(f\"Progreso: {round(count*100/len(dataframe), 2)}%\")\n            route = path/(str(dir_images) + \"/\" + str(name))\n            image = cv2.imread(os.path.join(route))\n            image = None\n            count += 1\"\"\"\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img_name = self.labels.iloc[idx, 0]\n        route = self.path/(str(self.path_images) + \"/\" + str(img_name))\n        image = PIL.Image.open(os.path.join(route))\n        im_label = self.labels.iloc[idx].values[1:].astype(float).reshape(-1,1) # reshape(-1,2) si son varias etiquetas\n        sample = {\"image\": image, \"label\": im_label}\n\n        if self.transforms:\n            sample[\"image\"] = self.transforms(sample[\"image\"])\n\n        return sample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating mean and std"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"folder = \"train_images\"\nto_tensor = transforms.Compose([\n        transforms.Resize(size2),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4307, 0.4967, 0.3135), (0.1960, 0.2011, 0.1767))\n    ])\n\ntrain_dataset = Dataset_from_csv(path, t, folder,to_tensor)\ndataloader = DataLoader(train_dataset, batch_size=int(len(t)*0.035), shuffle=False, num_workers=0)\n\nbatch = next(iter(dataloader))\nprint(batch[\"image\"].size())\nb = batch[\"image\"].view(batch[\"image\"].size(0), batch[\"image\"].size(1), -1)\nmean = (b.mean(2).sum(0))/batch[\"image\"].size()[0]\nstd = (b.std(2).sum(0))/batch[\"image\"].size()[0]\nprint(mean, std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mean: [0.0222, 0.0253, 0.0109] \n\nstd: [1.0582, 1.0520, 1.0556]"},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"### Pretrained EfficientNet with BCE"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n\ndef NewEfficientNet(c_out=1):\n    model = EfficientNet.from_pretrained('efficientnet-b5')\n    model._fc = nn.Linear(in_features=2048, out_features=c_out, bias=True)\n    return model\n\nmodel = NewEfficientNet(5)\nmodel = model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(y_pred, y):\n    n_true = torch.sum(y_pred.data.max(1, keepdim=True)[1].squeeze(-1) == y).item()\n    n_samples = y.size()[0]\n    return n_true, n_samples\n\ndef multilabel_accuracy(y_pred, y, threshold):\n    n_true, n_samples = 0, len(y_pred)\n    for pred,target in zip(y_pred, y):\n        if len(pred[pred>threshold])>1:\n            pred_label = (pred==(pred[pred>threshold].max())).nonzero()\n        else:\n            pred_label = ((pred>threshold)==True).nonzero()\n        label = (target==True).nonzero()\n        if str(pred_label)==str(label): n_true += 1\n    return n_true, n_samples\n\nclass Learner():\n    def __init__(self, model, train, valid, optim, loss_fn):\n        self.model = model\n        self.train = train\n        self.valid = valid\n        self.optim = optim\n        self.loss_fn = loss_fn\n        self.lr = 0\n        self.losses_train = []\n        self.losses_valid = []\n\n    def fit(self, epochs, lr, max_lr, threshold):\n        #scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optim, max_lr=max_lr, steps_per_epoch=len(self.train), epochs=epochs)\n        if lr != self.lr:\n            self.lr == lr\n            for param_group in self.optim.param_groups:\n                param_group['lr'] = lr\n        \n        for epoch in range(epochs):\n            sum_loss = 0\n            acc = 0\n            correct = 0\n            n_samples = 0\n            \n            acc_loss_val = 0\n\n            # Training\n            self.model = self.model.train()\n            for count, sample in enumerate(self.train):\n                X_batch = sample[\"image\"].to(\"cuda:0\")\n                y_batch = sample[\"label\"].long().to(\"cuda:0\")\n                y_pred = self.model(X_batch)                \n                loss = self.loss_fn(y_pred, y_batch.squeeze(2).squeeze(1))\n                correct += (y_batch.squeeze(2).squeeze(1) == y_pred.argmax(axis=1)).sum().item()\n                n_samples += len(y_batch.squeeze(2))\n \n                if count % 200 == 0:\n                    print(f\"Train epoch {epoch+1}: [{count}/{len(self.train)}\\tLoss: {round(loss.item(), 3)}\\tAcc: {round((correct/n_samples)*100, 2)}%]\")\n                sum_loss += loss.item()\n                acc_train = correct/n_samples\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n                #scheduler.step()\n\n            # Validation\n            self.model = self.model.eval()\n            correct = 0\n            n_samples = 0\n            with torch.no_grad():\n                for sample_val in self.valid:\n                    X_batch = sample_val[\"image\"].to(\"cuda:0\")\n                    y_batch = sample_val[\"label\"].long().to(\"cuda:0\")\n                    y_pred = self.model(X_batch)\n                    loss = self.loss_fn(y_pred, y_batch.squeeze(2).squeeze(1))\n                    correct += (y_batch.squeeze(2).squeeze(1) == y_pred.argmax(axis=1)).sum().item()\n                    n_samples += len(y_batch.squeeze(2).argmax(axis=1))\n                    acc_loss_val += loss.item()\n            acc_val = correct/n_samples\n            print(f\"Train epoch {epoch+1}: Train -> [Loss: {round(sum_loss/len(self.train), 3)}\\tAcc: {round(acc_train*100, 2)}%], Valid -> [Loss: {round(acc_loss_val/len(self.valid), 3)}\\tAcc: {round(acc_val*100, 2)}%]\\n\")\n            self.losses_train.append(sum_loss/len(self.train))\n            self.losses_valid.append(acc_loss_val/len(self.valid))\n\n    def plot(self):\n        plt.plot(learner.losses_train)\n        plt.plot(learner.losses_valid)\n        plt.ylabel('Error')\n        plt.xlabel('Epochs')\n        plt.legend([\"Entrenamiento\", \"Validación\"])\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size1 = [600, 800]\nsize2 = [300, 400]\nsize3 = [150, 200]\n\nsize_efficientnet = [528, 528]\n\ncurrent_size = size_efficientnet\n\ntsfm = transforms.Compose([\n        transforms.CenterCrop(current_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n        transforms.RandomRotation(20),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4307, 0.4967, 0.3135), (0.1960, 0.2011, 0.1767))\n    ])\n\ntsfm2 = transforms.Compose([\n        transforms.CenterCrop(current_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4307, 0.4967, 0.3135), (0.1960, 0.2011, 0.1767))\n    ])\n\nfolder = \"train\"\ntrain_dataset = Dataset_from_csv(path2, t, folder,tsfm)\nvalid_dataset = Dataset_from_csv(path2, v, folder,tsfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_size1 = [4,8]\nb_size2 = [6,12]\nb_size3 = [104,208]\n\nb_size = b_size1\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=b_size[0], shuffle=True, num_workers=0)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=b_size[1], shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model.load_state_dict(torch.load(\"./model_3_\"))\nlearner.model = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = Learner(model, train_dataloader, valid_dataloader, optimizer, loss_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.train=train_dataloader\nlearner.valid=valid_dataloader\nlearner.fit(2, 0.00003, 0.1, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit(2, 0.000005, 0.1, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit(1, 0.000001, 0.1, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"92.6%"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}